name: Price Scraper

on:
  schedule:
    # Run every 6 hours (00:00, 06:00, 12:00, 18:00 UTC)
    - cron: '0 */6 * * *'
  workflow_dispatch:  # Allow manual trigger

jobs:
  scrape-and-update:
    runs-on: ubuntu-latest
    permissions:
      contents: write  # Needed to commit the database back to the repo

    steps:
    - uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Install Playwright browsers
      run: |
        playwright install chromium
        playwright install-deps

    - name: Run Scraper (Canada Computers - All)
      env:
        DISCORD_WEBHOOK_URL: ${{ secrets.DISCORD_WEBHOOK_URL }}
      run: |
        python run_anomaly_detector.py --site canadacomputers --category all --pages 5

    - name: Run Scraper (Newegg - Deels/GPUs)
      env:
        DISCORD_WEBHOOK_URL: ${{ secrets.DISCORD_WEBHOOK_URL }}
      run: |
        # Newegg "all" might be too heavy/slow for GitHub Actions timeout, maybe stick to key categories or 'all' with low page count
        python run_anomaly_detector.py --site newegg --category all --pages 3

    - name: Run Scraper (Memory Express - All)
      env:
        DISCORD_WEBHOOK_URL: ${{ secrets.DISCORD_WEBHOOK_URL }}
      run: |
        python run_anomaly_detector.py --site memoryexpress --category all --pages 5

    - name: Commit and Push Database Changes
      run: |
        git config --global user.name 'GitHub Action'
        git config --global user.email 'action@github.com'
        git add prices.db
        # Only commit if db has changed
        if git diff --staged --quiet; then
          echo "No changes to commit"
        else
          git commit -m "Update prices.db [skip ci]"
          git push
        fi
